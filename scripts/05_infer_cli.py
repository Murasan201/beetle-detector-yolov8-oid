#!/usr/bin/env python3
"""
Beetle Detection æ¨è«–CLI
å­¦ç¿’æ¸ˆã¿YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‹ã‚‰Beetleã‚’æ¤œå‡º
"""

import os
import argparse
import json
from pathlib import Path
from typing import List, Dict, Any
import time

import cv2
import numpy as np
from PIL import Image
import torch
from ultralytics import YOLO

def load_model(model_path: str) -> YOLO:
    """
    å­¦ç¿’æ¸ˆã¿YOLOv8ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    
    Args:
        model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ (.pt)
        
    Returns:
        YOLO: èª­ã¿è¾¼ã¿æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    print(f"Loading model: {model_path}")
    model = YOLO(model_path)
    print(f"âœ… Model loaded successfully")
    print(f"   Device: {'GPU' if next(model.model.parameters()).is_cuda else 'CPU'}")
    print(f"   Classes: {model.names}")
    
    return model

def process_single_image(model: YOLO, image_path: str, conf: float = 0.25, 
                        save_result: bool = True, output_dir: str = "inference_results") -> Dict[str, Any]:
    """
    å˜ä¸€ç”»åƒã§ã®æ¨è«–å®Ÿè¡Œ
    
    Args:
        model: YOLOv8ãƒ¢ãƒ‡ãƒ«
        image_path: å…¥åŠ›ç”»åƒãƒ‘ã‚¹
        conf: ä¿¡é ¼åº¦é–¾å€¤
        save_result: çµæœç”»åƒã‚’ä¿å­˜ã™ã‚‹ã‹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        Dict: æ¤œå‡ºçµæœæƒ…å ±
    """
    
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Image file not found: {image_path}")
    
    # æ¨è«–å®Ÿè¡Œ
    start_time = time.time()
    results = model(image_path, conf=conf, verbose=False)
    inference_time = time.time() - start_time
    
    result = results[0]
    
    # æ¤œå‡ºçµæœã®è§£æ
    detections = []
    if result.boxes is not None:
        boxes = result.boxes
        for i, box in enumerate(boxes):
            detection = {
                "class_id": int(box.cls.cpu().numpy()[0]),
                "class_name": model.names[int(box.cls.cpu().numpy()[0])],
                "confidence": float(box.conf.cpu().numpy()[0]),
                "bbox": box.xyxy.cpu().numpy()[0].tolist(),  # [x1, y1, x2, y2]
            }
            detections.append(detection)
    
    # çµæœç”»åƒä¿å­˜
    output_path = None
    if save_result and detections:
        os.makedirs(output_dir, exist_ok=True)
        
        # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ãç”»åƒç”Ÿæˆ
        annotated_img = result.plot()
        
        # ä¿å­˜ãƒ‘ã‚¹ç”Ÿæˆ
        input_name = Path(image_path).stem
        output_path = os.path.join(output_dir, f"{input_name}_result.jpg")
        
        # BGR -> RGBå¤‰æ›ã—ã¦ä¿å­˜
        cv2.imwrite(output_path, annotated_img)
    
    # çµæœæƒ…å ±
    result_info = {
        "image_path": image_path,
        "inference_time_ms": inference_time * 1000,
        "num_detections": len(detections),
        "detections": detections,
        "output_path": output_path
    }
    
    return result_info

def process_batch_images(model: YOLO, image_paths: List[str], conf: float = 0.25,
                        save_results: bool = True, output_dir: str = "inference_results") -> List[Dict[str, Any]]:
    """
    è¤‡æ•°ç”»åƒã§ã®ä¸€æ‹¬æ¨è«–
    
    Args:
        model: YOLOv8ãƒ¢ãƒ‡ãƒ«
        image_paths: å…¥åŠ›ç”»åƒãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
        conf: ä¿¡é ¼åº¦é–¾å€¤
        save_results: çµæœç”»åƒã‚’ä¿å­˜ã™ã‚‹ã‹
        output_dir: å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        
    Returns:
        List[Dict]: å„ç”»åƒã®æ¤œå‡ºçµæœæƒ…å ±
    """
    
    print(f"=== Batch inference: {len(image_paths)} images ===")
    
    all_results = []
    total_detections = 0
    total_time = 0
    
    for i, image_path in enumerate(image_paths, 1):
        try:
            print(f"Processing [{i}/{len(image_paths)}]: {Path(image_path).name}")
            
            result_info = process_single_image(
                model, image_path, conf, save_results, output_dir
            )
            
            all_results.append(result_info)
            total_detections += result_info["num_detections"]
            total_time += result_info["inference_time_ms"]
            
            print(f"  Detections: {result_info['num_detections']}, "
                  f"Time: {result_info['inference_time_ms']:.2f}ms")
            
        except Exception as e:
            print(f"  âŒ Error processing {image_path}: {e}")
            continue
    
    # çµ±è¨ˆæƒ…å ±
    avg_time = total_time / len(all_results) if all_results else 0
    avg_fps = 1000 / avg_time if avg_time > 0 else 0
    
    print(f"\n=== Batch Results ===")
    print(f"Processed images: {len(all_results)}/{len(image_paths)}")
    print(f"Total detections: {total_detections}")
    print(f"Average inference time: {avg_time:.2f}ms")
    print(f"Average FPS: {avg_fps:.2f}")
    
    return all_results

def save_results_json(results: List[Dict[str, Any]], output_path: str):
    """
    æ¨è«–çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    
    Args:
        results: æ¨è«–çµæœã®ãƒªã‚¹ãƒˆ
        output_path: å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    """
    
    # ã‚µãƒãƒªãƒ¼æƒ…å ±è¿½åŠ 
    summary = {
        "total_images": len(results),
        "total_detections": sum(r["num_detections"] for r in results),
        "avg_inference_time_ms": np.mean([r["inference_time_ms"] for r in results]),
        "processing_timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    output_data = {
        "summary": summary,
        "results": results
    }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… Results saved to: {output_path}")

def get_image_files(source: str) -> List[str]:
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
    
    Args:
        source: ãƒ•ã‚¡ã‚¤ãƒ«/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãƒ‘ã‚¹ã€ã¾ãŸã¯ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰
        
    Returns:
        List[str]: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
    """
    
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
    image_files = []
    
    source_path = Path(source)
    
    if source_path.is_file():
        # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«
        if source_path.suffix.lower() in image_extensions:
            image_files.append(str(source_path))
    elif source_path.is_dir():
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        for ext in image_extensions:
            image_files.extend(source_path.glob(f"*{ext}"))
            image_files.extend(source_path.glob(f"*{ext.upper()}"))
        image_files = [str(f) for f in image_files]
    else:
        # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        from glob import glob
        image_files = glob(source)
        image_files = [f for f in image_files if Path(f).suffix.lower() in image_extensions]
    
    return sorted(image_files)

def main():
    parser = argparse.ArgumentParser(
        description="Beetle Detection Inference CLI",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # å˜ä¸€ç”»åƒã§ã®æ¨è«–
  python scripts/05_infer_cli.py --model runs/detect/train/weights/best.pt --source test.jpg

  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ç”»åƒã§æ¨è«–  
  python scripts/05_infer_cli.py --model runs/detect/train/weights/best.pt --source images/

  # ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰æŒ‡å®š
  python scripts/05_infer_cli.py --model runs/detect/train/weights/best.pt --source "images/*.jpg"

  # ä¿¡é ¼åº¦é–¾å€¤èª¿æ•´
  python scripts/05_infer_cli.py --model best.pt --source images/ --conf 0.5

  # çµæœä¿å­˜ãªã—ï¼ˆé€Ÿåº¦é‡è¦–ï¼‰
  python scripts/05_infer_cli.py --model best.pt --source images/ --no-save
        """
    )
    
    parser.add_argument("--model", "-m", required=True,
                       help="Path to trained YOLOv8 model (.pt)")
    parser.add_argument("--source", "-s", required=True,
                       help="Input source (image file, directory, or wildcard)")
    parser.add_argument("--conf", type=float, default=0.25,
                       help="Confidence threshold (default: 0.25)")
    parser.add_argument("--output-dir", "-o", default="inference_results",
                       help="Output directory for results (default: inference_results)")
    parser.add_argument("--no-save", action="store_true",
                       help="Do not save annotated images (faster)")
    parser.add_argument("--save-json", action="store_true",
                       help="Save results as JSON file")
    parser.add_argument("--device", default="",
                       help="Device to use (cpu, 0, 1, etc.). Auto-detect if empty")
    
    args = parser.parse_args()
    
    try:
        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        model = load_model(args.model)
        
        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        if args.device:
            model.to(args.device)
            print(f"Using device: {args.device}")
        
        # å…¥åŠ›ç”»åƒå–å¾—
        image_files = get_image_files(args.source)
        
        if not image_files:
            print(f"âŒ No image files found in: {args.source}")
            return 1
            
        print(f"Found {len(image_files)} image(s)")
        
        # æ¨è«–å®Ÿè¡Œ
        save_results = not args.no_save
        
        if len(image_files) == 1:
            # å˜ä¸€ç”»åƒ
            result = process_single_image(
                model, image_files[0], args.conf, save_results, args.output_dir
            )
            results = [result]
            
            print(f"\n=== Results ===")
            print(f"Image: {result['image_path']}")
            print(f"Detections: {result['num_detections']}")
            print(f"Inference time: {result['inference_time_ms']:.2f}ms")
            
            if result['detections']:
                print("Detected objects:")
                for i, det in enumerate(result['detections'], 1):
                    print(f"  {i}. {det['class_name']}: {det['confidence']:.3f}")
            
        else:
            # è¤‡æ•°ç”»åƒ
            results = process_batch_images(
                model, image_files, args.conf, save_results, args.output_dir
            )
        
        # JSONä¿å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if args.save_json:
            json_path = os.path.join(args.output_dir, "inference_results.json")
            os.makedirs(args.output_dir, exist_ok=True)
            save_results_json(results, json_path)
        
        print(f"\nâœ… Inference completed successfully!")
        if save_results:
            print(f"ğŸ“ Results saved in: {args.output_dir}")
        
        return 0
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return 1

if __name__ == "__main__":
    exit(main())