#!/usr/bin/env python3
"""
FiftyOneãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰YOLOå½¢å¼ã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå°ã•ã„ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹é™¤å»ï¼‰ã‚‚å«ã‚€
"""

import os
import argparse
import fiftyone as fo
from pathlib import Path
import pandas as pd

def filter_small_boxes(dataset, min_side=8, min_area=64):
    """
    å°ã•ã™ãã‚‹ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ã‚’é™¤å»
    
    Args:
        dataset: FiftyOne dataset
        min_side (int): æœ€å°ã‚µã‚¤ãƒ‰ã‚µã‚¤ã‚ºï¼ˆãƒ”ã‚¯ã‚»ãƒ«ï¼‰
        min_area (int): æœ€å°é¢ç©ï¼ˆãƒ”ã‚¯ã‚»ãƒ«^2ï¼‰
    """
    print(f"=== å°ã•ã„ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹é™¤å»é–‹å§‹ ===")
    print(f"Min side: {min_side}px, Min area: {min_area}pxÂ²")
    
    to_delete = []
    total_boxes_before = 0
    total_boxes_after = 0
    
    for sample in dataset:
        det = sample.detections
        if not det:
            to_delete.append(sample.id)
            continue
            
        total_boxes_before += len(det.detections)
        keep = []
        
        for d in det.detections:
            # æ­£è¦åŒ–åº§æ¨™ã‹ã‚‰å®Ÿéš›ã®ãƒ”ã‚¯ã‚»ãƒ«åº§æ¨™ã«å¤‰æ›
            w = d.bounding_box[2] * sample.metadata.width
            h = d.bounding_box[3] * sample.metadata.height
            area = w * h
            
            # ã‚µã‚¤ã‚ºã¨é¢ç©ã®æ¡ä»¶ã‚’ãƒã‚§ãƒƒã‚¯
            if w >= min_side and h >= min_side and area >= min_area:
                keep.append(d)
        
        if keep:
            det.detections = keep
            sample.detections = det
            sample.save()
            total_boxes_after += len(keep)
        else:
            # ã™ã¹ã¦ã®ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹ãŒå°ã•ã™ãã‚‹å ´åˆã¯å‰Šé™¤
            to_delete.append(sample.id)
    
    # ä¸è¦ãªã‚µãƒ³ãƒ—ãƒ«ã‚’å‰Šé™¤
    if to_delete:
        dataset.delete_samples(to_delete)
    
    print(f"å‰Šé™¤ã•ã‚ŒãŸã‚µãƒ³ãƒ—ãƒ«æ•°: {len(to_delete)}")
    print(f"ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹: {total_boxes_before} â†’ {total_boxes_after}")
    print(f"é™¤å»ç‡: {((total_boxes_before - total_boxes_after) / total_boxes_before * 100):.1f}%")
    
    return len(to_delete), total_boxes_before - total_boxes_after

def export_to_yolo(train_dataset, val_dataset, export_dir="datasets/beetle-oid-yolo"):
    """
    YOLOå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    
    Args:
        train_dataset: è¨“ç·´ç”¨FiftyOne dataset
        val_dataset: æ¤œè¨¼ç”¨FiftyOne dataset  
        export_dir: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    """
    print(f"=== YOLOå½¢å¼ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆé–‹å§‹ ===")
    print(f"Export directory: {export_dir}")
    
    export_path = Path(export_dir)
    export_path.mkdir(parents=True, exist_ok=True)
    
    try:
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        print("\n--- è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ ---")
        train_dataset.export(
            export_dir=str(export_path),
            dataset_type=fo.types.YOLOv5Dataset,
            label_field="detections",
            split="train",
            classes=["Beetle"],
        )
        
        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        print("--- æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆä¸­ ---")
        val_dataset.export(
            export_dir=str(export_path), 
            dataset_type=fo.types.YOLOv5Dataset,
            label_field="detections", 
            split="val",
            classes=["Beetle"],
        )
        
        # data.yamlãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèªãƒ»ä¿®æ­£
        data_yaml = export_path / "data.yaml"
        if data_yaml.exists():
            print(f"âœ… data.yaml created: {data_yaml}")
            
            # data.yamlã®å†…å®¹ã‚’è¡¨ç¤º
            with open(data_yaml, 'r') as f:
                content = f.read()
                print("data.yaml content:")
                print(content)
        else:
            print("âŒ data.yaml not found")
            
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ç¢ºèª
        print(f"\n=== ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº† ===")
        print("Directory structure:")
        for item in sorted(export_path.rglob("*")):
            if item.is_file():
                rel_path = item.relative_to(export_path)
                print(f"  {rel_path}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return False

def create_attribution_csv(train_dataset, val_dataset, output_path="docs/ATTRIBUTION.csv"):
    """
    ä½¿ç”¨ã—ãŸç”»åƒã®å¸°å±æƒ…å ±CSVã‚’ä½œæˆï¼ˆãƒ©ã‚¤ã‚»ãƒ³ã‚¹éµå®ˆç”¨ï¼‰
    
    Args:
        train_dataset: è¨“ç·´ç”¨dataset
        val_dataset: æ¤œè¨¼ç”¨dataset
        output_path: CSVå‡ºåŠ›ãƒ‘ã‚¹
    """
    print(f"=== å¸°å±æƒ…å ±CSVä½œæˆé–‹å§‹ ===")
    
    attribution_data = []
    
    # ä¸¡æ–¹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰æƒ…å ±ã‚’åé›†
    for split, dataset in [("train", train_dataset), ("val", val_dataset)]:
        for sample in dataset:
            # Open Imagesã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            image_id = sample.get("open_images_id", "unknown")
            # ä»–ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚‚åˆ©ç”¨å¯èƒ½ã§ã‚ã‚Œã°è¿½åŠ 
            attribution_data.append({
                "split": split,
                "image_id": image_id,
                "filename": os.path.basename(sample.filepath),
                "source": "Open Images Dataset V7",
                "license": "CC BY 2.0 (individual verification required)",
                "attribution": "Image from Open Images Dataset V7, licensed under CC BY 2.0",
                "url": f"https://storage.googleapis.com/openimages/web/visualizer/index.html?set=train&type=detection&c=%2Fm%2F0cyf8&id={image_id}",
                "usage_date": pd.Timestamp.now().strftime("%Y-%m-%d")
            })
    
    # CSVä½œæˆ
    df = pd.DataFrame(attribution_data)
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(output_path, index=False)
    
    print(f"âœ… å¸°å±æƒ…å ±CSVä½œæˆå®Œäº†: {output_path}")
    print(f"   Total records: {len(attribution_data)}")
    
    return output_path

def main():
    parser = argparse.ArgumentParser(description="Clean and export FiftyOne datasets to YOLO format")
    parser.add_argument("--train-dataset", default="oid_beetle_train_200",
                       help="Name of train dataset in FiftyOne")
    parser.add_argument("--val-dataset", default="oid_beetle_val_50", 
                       help="Name of val dataset in FiftyOne")
    parser.add_argument("--export-dir", default="datasets/beetle-oid-yolo",
                       help="Export directory for YOLO format")
    parser.add_argument("--min-side", type=int, default=8,
                       help="Minimum side length for bounding boxes (pixels)")
    parser.add_argument("--min-area", type=int, default=64,
                       help="Minimum area for bounding boxes (pixels^2)")
    parser.add_argument("--skip-cleaning", action="store_true",
                       help="Skip data cleaning step")
    parser.add_argument("--full-scale", action="store_true",
                       help="Use full scale dataset names")
    
    args = parser.parse_args()
    
    # ãƒ•ãƒ«ã‚¹ã‚±ãƒ¼ãƒ«ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåã‚’èª¿æ•´
    if args.full_scale:
        args.train_dataset = "oid_beetle_train_1200"
        args.val_dataset = "oid_beetle_val_300"
        print("ğŸš€ Full scale mode enabled")
    
    print(f"Train dataset: {args.train_dataset}")
    print(f"Val dataset: {args.val_dataset}")
    
    try:
        # FiftyOneãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
        print("=== FiftyOneãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ä¸­ ===")
        train_dataset = fo.load_dataset(args.train_dataset)
        val_dataset = fo.load_dataset(args.val_dataset)
        
        print(f"Train samples: {len(train_dataset)}")
        print(f"Val samples: {len(val_dataset)}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if not args.skip_cleaning:
            print("\n--- ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ ---")
            deleted_train, removed_boxes_train = filter_small_boxes(
                train_dataset, args.min_side, args.min_area
            )
            deleted_val, removed_boxes_val = filter_small_boxes(
                val_dataset, args.min_side, args.min_area  
            )
            
            print(f"ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¾Œ - Train: {len(train_dataset)}, Val: {len(val_dataset)}")
        else:
            print("â­ï¸  ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—")
        
        # YOLOå½¢å¼ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        success = export_to_yolo(train_dataset, val_dataset, args.export_dir)
        
        if success:
            # å¸°å±æƒ…å ±CSVä½œæˆï¼ˆãƒ©ã‚¤ã‚»ãƒ³ã‚¹éµå®ˆï¼‰
            attribution_path = create_attribution_csv(train_dataset, val_dataset)
            
            print("âœ… å…¨å‡¦ç†å®Œäº†ï¼")
            print("æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: bash scripts/03_train_yolov8.sh")
            return 0
        else:
            print("âŒ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¤±æ•—")
            return 1
            
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("FiftyOneãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        print("åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:")
        for name in fo.list_datasets():
            print(f"  - {name}")
        return 1

if __name__ == "__main__":
    exit(main())